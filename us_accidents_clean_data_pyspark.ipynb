{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3324d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from typing import Dict, Any, Optional, List\n",
    "from pyspark import RDD\n",
    "\n",
    "def clean_us_accidents_pyspark(\n",
    "    rows: RDD[Dict[str, Any]],         \n",
    "    header_cols: List[str],         \n",
    "    output_dir: str,\n",
    ") -> None:\n",
    "    sc = rows.context\n",
    "\n",
    "    US_STATES = {\n",
    "        \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\n",
    "        \"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\n",
    "        \"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\",\"DC\"\n",
    "    }\n",
    "\n",
    "    WEATHER_BOUNDS = {\n",
    "        \"Temperature(F)\": (-70.0, 130.0),\n",
    "        \"Wind_Chill(F)\": (-50.0, 130.0),\n",
    "        \"Humidity(%)\": (0.0, 100.0),\n",
    "        \"Pressure(in)\": (15.0, 32.0),\n",
    "        \"Visibility(mi)\": (0.0, 100.0),\n",
    "        \"Wind_Speed(mph)\": (0.0, 200.0),\n",
    "        \"Precipitation(in)\": (0.0, 50.0),\n",
    "    }\n",
    "\n",
    "    COLS_TO_DROP = {\"ID\", \"Source\", \"Zipcode\", \"Timezone\", \"Airport_Code\", \"End_Lat\", \"End_Lng\"}\n",
    "\n",
    "    WIND_DIR_MAP = {\n",
    "        \"VARIABLE\": \"VAR\",\n",
    "        \"VAR\": \"VAR\",\n",
    "        \"CALM\": \"CALM\",\n",
    "        \"NORTH\": \"N\",\n",
    "        \"SOUTH\": \"S\",\n",
    "        \"EAST\": \"E\",\n",
    "        \"WEST\": \"W\",\n",
    "    }\n",
    "\n",
    "    TWILIGHT_COLS = [\"Sunrise_Sunset\",\"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\"]\n",
    "\n",
    "    BOOL_COLS = [\n",
    "        \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
    "        \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
    "    ]\n",
    "\n",
    "    def strip(s: Any) -> Optional[str]:\n",
    "        if s is None:\n",
    "            return None\n",
    "        s = str(s).strip()\n",
    "        return s if s != \"\" else None\n",
    "\n",
    "    def to_float(x: Any) -> Optional[float]:\n",
    "        sx = strip(x)\n",
    "        if sx is None:\n",
    "            return None\n",
    "        return float(sx)\n",
    "\n",
    "    def to_int_round(x: Any) -> Optional[int]:\n",
    "        f = to_float(x)\n",
    "        if f is None:\n",
    "            return None\n",
    "        return int(round(f))\n",
    "\n",
    "    def in_bounds(v: Optional[float], lo: float, hi: float) -> Optional[float]:\n",
    "        if v is None:\n",
    "            return None\n",
    "        if v < lo or v > hi:\n",
    "            return None\n",
    "        return v\n",
    "\n",
    "    def normalise_twilight(v: Any) -> str:\n",
    "        v = strip(v)\n",
    "        if v is None:\n",
    "            return \"Unknown\"\n",
    "        v = v.title()\n",
    "        return v if v in (\"Day\", \"Night\") else \"Unknown\"\n",
    "\n",
    "    def parse_bool(v: Any) -> Optional[bool]:\n",
    "        if v is None:\n",
    "            return None\n",
    "        if isinstance(v, bool):\n",
    "            return v\n",
    "        s = strip(v)\n",
    "        if s is None:\n",
    "            return None\n",
    "        s = s.lower()\n",
    "        if s == \"true\":\n",
    "            return True\n",
    "        if s == \"false\":\n",
    "            return False\n",
    "        return None\n",
    "\n",
    "    def clean_row(row: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        # Trim all strings\n",
    "        for k, v in list(row.items()):\n",
    "            if isinstance(v, str):\n",
    "                row[k] = v.strip()\n",
    "\n",
    "        if strip(row.get(\"Start_Time\")) is None:\n",
    "            return None\n",
    "\n",
    "        # Bound serverity from 1 to 4\n",
    "        if \"Severity\" in row:\n",
    "            severity = to_int_round(row.get(\"Severity\"))\n",
    "            if severity is None or severity < 1 or severity > 4:\n",
    "                return None\n",
    "            row[\"Severity\"] = severity\n",
    "        \n",
    "        # Bound coordinates and remove rows with misisng coordinates\n",
    "        lat = in_bounds(to_float(row.get(\"Start_Lat\")), -90.0, 90.0)\n",
    "        lng = in_bounds(to_float(row.get(\"Start_Lng\")), -180.0, 180.0)\n",
    "        if lat is None or lng is None:\n",
    "            return None\n",
    "        row[\"Start_Lat\"] = lat\n",
    "        row[\"Start_Lng\"] = lng\n",
    "\n",
    "        # Ensure that all rows have a valid US State\n",
    "        if \"State\" in row:\n",
    "            st = strip(row.get(\"State\"))\n",
    "            st = st.upper() if st else None\n",
    "            row[\"State\"] = st if (st in US_STATES) else None\n",
    "\n",
    "        # Bound weather data\n",
    "        for c, (lo, hi) in WEATHER_BOUNDS.items():\n",
    "            if c in row:\n",
    "                row[c] = in_bounds(to_float(row.get(c)), lo, hi)\n",
    "\n",
    "        # Standardise wind direction\n",
    "        if \"Wind_Direction\" in row:\n",
    "            wd = strip(row.get(\"Wind_Direction\"))\n",
    "            wd = wd.upper() if wd else None\n",
    "            row[\"Wind_Direction\"] = None if wd is None else WIND_DIR_MAP.get(wd, wd)\n",
    "\n",
    "        # Normalise twilight cols\n",
    "        for c in TWILIGHT_COLS:\n",
    "            if c in row:\n",
    "                row[c] = normalise_twilight(row.get(c))\n",
    "\n",
    "        # Assume all missing booleans to be false\n",
    "        for c in BOOL_COLS:\n",
    "            if c in row:\n",
    "                b = parse_bool(row.get(c))\n",
    "                row[c] = False if b is None else b\n",
    "\n",
    "        # Drop all redundant cols\n",
    "        for c in COLS_TO_DROP:\n",
    "            row.pop(c, None)\n",
    "\n",
    "        return row\n",
    "\n",
    "    # Remove duplicated IDs\n",
    "    def row_id(r: Dict[str, Any]) -> str:\n",
    "        rid = strip(r.get(\"ID\"))\n",
    "        return rid if rid is not None else \"\"\n",
    "\n",
    "    deduped = (\n",
    "        rows\n",
    "        .map(lambda r: (row_id(r), r))\n",
    "        .reduceByKey(lambda a, b: a)\n",
    "        .values()\n",
    "    )\n",
    "\n",
    "    cleaned = deduped.map(clean_row).filter(lambda r: r is not None).cache()\n",
    "\n",
    "    # Fill in missing weather data using median from sampled data\n",
    "    weather_cols = [c for c in WEATHER_BOUNDS.keys() if c in header_cols]\n",
    "    median_sample_frac= 0.02\n",
    "    median_min_samples = 50000\n",
    "    \n",
    "    if weather_cols:\n",
    "        sampled = (\n",
    "            cleaned\n",
    "            .sample(withReplacement=False, fraction=median_sample_frac, seed=1)\n",
    "            .take(median_min_samples)\n",
    "        )\n",
    "\n",
    "        medians: Dict[str, Optional[float]] = {}\n",
    "        for c in weather_cols:\n",
    "            vals = []\n",
    "            for r in sampled:\n",
    "                v = r.get(c)\n",
    "                if v is None:\n",
    "                    continue\n",
    "                fv = float(v)\n",
    "                if math.isnan(fv):\n",
    "                    continue\n",
    "                vals.append(fv)\n",
    "\n",
    "            if not vals:\n",
    "                medians[c] = None\n",
    "            else:\n",
    "                vals.sort()\n",
    "                n = len(vals)\n",
    "                mid = n // 2\n",
    "                medians[c] = vals[mid] if (n % 2 == 1) else (vals[mid - 1] + vals[mid]) / 2.0\n",
    "\n",
    "        medians_bc = sc.broadcast(medians)\n",
    "\n",
    "        def fill_weather_medians(r: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            m = medians_bc.value\n",
    "            for c in weather_cols:\n",
    "                if r.get(c) is None and m.get(c) is not None:\n",
    "                    r[c] = float(m[c])\n",
    "            return r\n",
    "\n",
    "        cleaned = cleaned.map(fill_weather_medians)\n",
    "    \n",
    "    # Write to output directory\n",
    "    cleaned.map(lambda r: json.dumps(r, ensure_ascii=False)).saveAsTextFile(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"clean-us-accidents\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "lines = sc.textFile(\"US_Accidents_March23_sampled_500k.csv\", minPartitions=8)\n",
    "header = lines.first()\n",
    "\n",
    "def parse_csv_line(line: str):\n",
    "    return next(csv.reader([line]))\n",
    "\n",
    "header_cols = parse_csv_line(header)\n",
    "rows_rdd = (\n",
    "    lines.filter(lambda x: x != header)\n",
    "         .map(parse_csv_line)\n",
    "         .map(lambda vals: dict(zip(header_cols, vals)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedc491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Please change the output dir accoridngly\n",
    "clean_us_accidents_pyspark(\n",
    "    rows=rows_rdd,\n",
    "    header_cols=header_cols,\n",
    "    output_dir=\"out/cleaned_jsonl\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us_accidents_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
