{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300f6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_HOME\"] = os.path.expanduser(\"~/hadoop\")\n",
    "os.environ[\"HADOOP_COMMON_HOME\"] = os.environ[\"HADOOP_HOME\"]\n",
    "os.environ[\"PATH\"] = os.environ[\"HADOOP_HOME\"] + \"/bin:\" + os.environ[\"HADOOP_HOME\"] + \"/sbin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1923e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/data/us_accidents/raw/US_Accidents_March23_sampled_500k.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "!hdfs dfs -mkdir -p /data/us_accidents/raw \n",
    "!hdfs dfs -put US_Accidents_March23_sampled_500k.csv /data/us_accidents/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3324d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import math\n",
    "from typing import Dict, Any, Optional, List\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def clean_us_accidents_spark_core(\n",
    "    spark: SparkSession,\n",
    "    hdfs_input_csv: str,\n",
    "    hdfs_output_dir: str,\n",
    "    min_partitions: int = 8,\n",
    ") -> None:\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    US_STATES = {\n",
    "    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\n",
    "    \"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\n",
    "    \"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\",\"DC\"\n",
    "}\n",
    "\n",
    "    WEATHER_BOUNDS = {\n",
    "        \"Temperature(F)\": (-70.0, 130.0),\n",
    "        \"Wind_Chill(F)\": (-50.0, 130.0),\n",
    "        \"Humidity(%)\": (0.0, 100.0),\n",
    "        \"Pressure(in)\": (15.0, 32.0),\n",
    "        \"Visibility(mi)\": (0.0, 100.0),\n",
    "        \"Wind_Speed(mph)\": (0.0, 200.0),\n",
    "        \"Precipitation(in)\": (0.0, 50.0),\n",
    "    }\n",
    "\n",
    "    COLS_TO_DROP = {\"ID\", \"Source\", \"Zipcode\", \"Timezone\", \"Airport_Code\", \"End_Lat\", \"End_Lng\"}\n",
    "\n",
    "    WIND_DIR_MAP = {\n",
    "        \"VARIABLE\": \"VAR\",\n",
    "        \"VAR\": \"VAR\",\n",
    "        \"CALM\": \"CALM\",\n",
    "        \"NORTH\": \"N\",\n",
    "        \"SOUTH\": \"S\",\n",
    "        \"EAST\": \"E\",\n",
    "        \"WEST\": \"W\",\n",
    "    }\n",
    "\n",
    "    TWILIGHT_COLS = [\"Sunrise_Sunset\",\"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\"]\n",
    "\n",
    "    BOOL_COLS = [\n",
    "        \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
    "        \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
    "    ]\n",
    "    \n",
    "    def to_float(x: Any) -> Optional[float]: \n",
    "        x = strip(x) \n",
    "        if x is None: return None \n",
    "        return float(x) \n",
    "\n",
    "    def to_int_round(x: Any) -> Optional[int]: \n",
    "        f = to_float(x) \n",
    "        if f is None: return None \n",
    "        return int(round(f))\n",
    "    \n",
    "    def strip(s: Any) -> Optional[str]:\n",
    "        if s is None:\n",
    "            return None\n",
    "        s = str(s).strip()\n",
    "        return s if s != \"\" else None\n",
    "\n",
    "    def in_bounds(v: Optional[float], lo: float, hi: float) -> Optional[float]:\n",
    "        if v is None:\n",
    "            return None\n",
    "        if v < lo or v > hi:\n",
    "            return None\n",
    "        return v\n",
    "\n",
    "\n",
    "    def normalise_twilight(v: Any) -> str:\n",
    "        v = strip(v)\n",
    "        if v is None:\n",
    "            return \"Unknown\"\n",
    "        v = v.title()\n",
    "        return v if v in (\"Day\", \"Night\") else \"Unknown\"\n",
    "\n",
    "    def parse_bool(v: Any) -> Optional[bool]:\n",
    "        if v is None:\n",
    "            return None\n",
    "        if isinstance(v, bool):\n",
    "            return v\n",
    "        s = strip(v)\n",
    "        if s is None:\n",
    "            return None\n",
    "        s = s.lower()\n",
    "        if s == \"true\":\n",
    "            return True\n",
    "        if s == \"false\":\n",
    "            return False\n",
    "        return None\n",
    "\n",
    "\n",
    "    def clean_row(row: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "        # Trim all strings\n",
    "        for k, v in list(row.items()):\n",
    "            if isinstance(v, str):\n",
    "                row[k] = v.strip()\n",
    "\n",
    "        # Remove rows with missing start time\n",
    "        if row.get(\"Start_Time\") is None:\n",
    "            return None\n",
    "\n",
    "        # Bound serverity\n",
    "        if \"Severity\" in row:\n",
    "            severity = to_int_round(row.get(\"Severity\"))\n",
    "            if severity is None or severity < 1 or severity > 4:\n",
    "                return None \n",
    "            row[\"Severity\"] = severity\n",
    "\n",
    "        # Bound coordinates\n",
    "        lat = in_bounds(to_float(row.get(\"Start_Lat\")), -90.0, 90.0)\n",
    "        lng = in_bounds(to_float(row.get(\"Start_Lng\")), -180.0, 180.0)\n",
    "        if lat is None or lng is None:\n",
    "            return None\n",
    "        \n",
    "        # Remove rows with missing start_lat and start_lng\n",
    "        row[\"Start_Lat\"] = lat\n",
    "        row[\"Start_Lng\"] = lng\n",
    "\n",
    "        # Filter states not in US_STATES\n",
    "        if \"State\" in row:\n",
    "            st = strip(row.get(\"State\"))\n",
    "            st = st.upper() if st else None\n",
    "            row[\"State\"] = st if (st in US_STATES) else None\n",
    "\n",
    "        # Bound weather data\n",
    "        for c, (lo, hi) in WEATHER_BOUNDS.items():\n",
    "            if c in row:\n",
    "                row[c] = in_bounds(to_float(row.get(c)), lo, hi)\n",
    "\n",
    "        # Standarize wind direction\n",
    "        if \"Wind_Direction\" in row:\n",
    "            wd = strip(row.get(\"Wind_Direction\"))\n",
    "            wd = wd.upper() if wd else None\n",
    "            if wd is None:\n",
    "                row[\"Wind_Direction\"] = None\n",
    "            else:\n",
    "                row[\"Wind_Direction\"] = WIND_DIR_MAP.get(wd, wd)\n",
    "\n",
    "        # Normalise twilight cols\n",
    "        for c in TWILIGHT_COLS:\n",
    "            if c in row:\n",
    "                row[c] = normalise_twilight(row.get(c))\n",
    "\n",
    "        # Assume missing booleans as false\n",
    "        for c in BOOL_COLS:\n",
    "            if c in row:\n",
    "                b = parse_bool(row.get(c))\n",
    "                row[c] = False if b is None else b\n",
    "\n",
    "        # Drop redundant columns\n",
    "        for c in COLS_TO_DROP:\n",
    "            row.pop(c, None)\n",
    "\n",
    "        return row\n",
    "\n",
    "    # Read raw CSV lines from HDFS\n",
    "    lines = sc.textFile(hdfs_input_csv, minPartitions=min_partitions)\n",
    "    header = lines.first()\n",
    "\n",
    "    # CSV parsing helper\n",
    "    def parse_csv_line(line: str) -> List[str]:\n",
    "        return next(csv.reader([line]))\n",
    "\n",
    "    header_cols = parse_csv_line(header)\n",
    "    data_lines = lines.filter(lambda x: x != header)\n",
    "    rows = data_lines.map(parse_csv_line).map(lambda vals: dict(zip(header_cols, vals)))\n",
    "\n",
    "    # Drop duplicated IDs\n",
    "    def row_id(r: Dict[str, Any]) -> str:\n",
    "        rid = strip(r.get(\"ID\"))\n",
    "        return rid if rid is not None else \"\"\n",
    "\n",
    "    keyed = rows.map(lambda r: (row_id(r), r))\n",
    "    deduped = keyed.reduceByKey(lambda a, b: a).values()\n",
    "\n",
    "    # Clean rows\n",
    "    cleaned = deduped.map(clean_row).filter(lambda r: r is not None)\n",
    "\n",
    "    weather_cols = [c for c in WEATHER_BOUNDS.keys() if c in header_cols]\n",
    "\n",
    "    if len(weather_cols) > 0:\n",
    "        # Emit (col, value) for non-missing weather values\n",
    "        def emit_weather_vals(r: Dict[str, Any]):\n",
    "            for c in weather_cols:\n",
    "                v = r.get(c)\n",
    "                if v is None:\n",
    "                    continue\n",
    "                fv = float(v)\n",
    "                if not math.isnan(fv):\n",
    "                    yield (c, fv)\n",
    "\n",
    "        col_to_vals = (\n",
    "            cleaned\n",
    "            .flatMap(emit_weather_vals)\n",
    "            .groupByKey()\n",
    "            .mapValues(lambda it: list(it))\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        # Compute medians\n",
    "        medians = {}\n",
    "        for c, vals in col_to_vals:\n",
    "            if not vals:\n",
    "                medians[c] = None\n",
    "                continue\n",
    "            vals.sort()\n",
    "            n = len(vals)\n",
    "            mid = n // 2\n",
    "            if n % 2 == 1:\n",
    "                medians[c] = vals[mid]\n",
    "            else:\n",
    "                medians[c] = (vals[mid - 1] + vals[mid]) / 2.0\n",
    "\n",
    "        medians_bc = sc.broadcast(medians)\n",
    "\n",
    "        # Fill missing values with median\n",
    "        def fill_weather_medians(r: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            m = medians_bc.value\n",
    "            for c in weather_cols:\n",
    "                if r.get(c) is None and m.get(c) is not None:\n",
    "                    r[c] = float(m[c])\n",
    "            return r\n",
    "    cleaned = cleaned.map(fill_weather_medians)\n",
    "    \n",
    "    # Write JSONL to HDFS \n",
    "    cleaned.map(lambda r: json.dumps(r, ensure_ascii=False)).saveAsTextFile(hdfs_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\") \n",
    "    .config(\"spark.python.worker.reuse\", \"true\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Please change input and output directory \n",
    "clean_us_accidents_spark_core(\n",
    "    spark,\n",
    "    hdfs_input_csv=\"hdfs://LAPTOP-5DR87JF2:9900/data/us_accidents/raw/US_Accidents_March23_sampled_500k.csv\",\n",
    "    hdfs_output_dir=\"hdfs://LAPTOP-5DR87JF2:9900/data/us_accidents/cleaned_dedup_jsonl\",\n",
    "    min_partitions=8,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us_accidents_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
