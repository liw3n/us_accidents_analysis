{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcca4ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"US_Accidents_March23.csv\")   \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae21371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "US_STATES = {\n",
    "    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\n",
    "    \"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\n",
    "    \"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\",\"DC\"\n",
    "}\n",
    "\n",
    "def clean_us_accidents(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove whitespaces in strings\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    for c in obj_cols:\n",
    "        df[c] = df[c].astype(\"string\").str.strip()\n",
    "        \n",
    "    # Drop rows with duplicated IDs\n",
    "    if \"ID\" in df.columns:\n",
    "        df[\"ID\"] = df[\"ID\"].astype(\"string\")\n",
    "        df = df.drop_duplicates(subset=[\"ID\"])\n",
    "        \n",
    "    # Remove redundant columns\n",
    "    COLS_TO_DROP = [\"ID\", \"Source\", \"Zipcode\", \"Timezone\", \"Airport_Code\", \"End_Lat\", \"End_Lng\"]\n",
    "    df = df.drop(columns=COLS_TO_DROP, errors=\"ignore\")\n",
    "    \n",
    "    # Replace wrong/missing values as NaT\n",
    "    for c in [\"Start_Time\", \"End_Time\", \"Weather_Timestamp\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Remove rows with missing start time values\n",
    "    df = df[df[\"Start_Time\"].notna()]\n",
    "    \n",
    "    def remove_out_of_bounds_values(col, lo=None, hi=None):\n",
    "        if col not in df.columns:\n",
    "            return\n",
    "        \n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        if lo is not None:\n",
    "            df.loc[df[col] < lo, col] = np.nan\n",
    "        if hi is not None:\n",
    "            df.loc[df[col] > hi, col] = np.nan\n",
    "        \n",
    "            \n",
    "    # Remove out of bounds values for severity\n",
    "    remove_out_of_bounds_values(\"Severity\", 1, 4)\n",
    "    df[\"Severity\"] = df[\"Severity\"].round().astype(\"Int64\")\n",
    "\n",
    "    # Remove out of bounds values for coordinates\n",
    "    remove_out_of_bounds_values(\"Start_Lat\", -90, 90)\n",
    "    remove_out_of_bounds_values(\"Start_Lng\", -180, 180)\n",
    "    \n",
    "    # Remove missing Start_Lat or Start_Lng rows\n",
    "    df = df[df[\"Start_Lat\"].notna() & df[\"Start_Lng\"].notna()]\n",
    "    \n",
    "    # Set all in State to capital letters and set states which are not in US_STATES to NA\n",
    "    df[\"State\"] = df[\"State\"].astype(\"string\").str.upper()\n",
    "    df.loc[~df[\"State\"].isin(US_STATES), \"State\"] = pd.NA\n",
    "    df[\"State\"] = df[\"State\"].astype(\"category\")\n",
    "    \n",
    "    WEATHER_BOUNDS = {\n",
    "        \"Temperature(F)\": (-70, 130),\n",
    "        \"Wind_Chill(F)\": (-50, 130),\n",
    "        \"Humidity(%)\": (0, 100),\n",
    "        \"Pressure(in)\": (15, 32),\n",
    "        \"Visibility(mi)\": (0, 100),\n",
    "        \"Wind_Speed(mph)\": (0, 200),\n",
    "        \"Precipitation(in)\": (0, 50),\n",
    "    }\n",
    "\n",
    "    # Apply bounds to weather data\n",
    "    for col, (lo, hi) in WEATHER_BOUNDS.items():\n",
    "        remove_out_of_bounds_values(col, lo, hi)\n",
    "\n",
    "    weather_num = [c for c in WEATHER_BOUNDS.keys() if c in df.columns]\n",
    "    median_sample_frac = 0.02\n",
    "    median_min_samples = 50000\n",
    "\n",
    "    # Fill missing values using sampled medians\n",
    "    if len(weather_num) >= 1:\n",
    "        weather_df = df[weather_num].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "        n = len(weather_df)\n",
    "        # Choose sample\n",
    "        sample_n = int(min(n, max(median_min_samples, round(median_sample_frac * n))))\n",
    "\n",
    "        if sample_n < n:\n",
    "            sample_df = weather_df.sample(n=sample_n, random_state=0)\n",
    "        else:\n",
    "            sample_df = weather_df\n",
    "\n",
    "        medians = sample_df.median(skipna=True)\n",
    "\n",
    "        # fill missing data in dataset with sampled medians\n",
    "        df[weather_num] = weather_df.fillna(medians)\n",
    "\n",
    "    # Standarize wind directions\n",
    "    df[\"Wind_Direction\"] = df[\"Wind_Direction\"].astype(\"string\").str.strip().str.upper()\n",
    "    df[\"Wind_Direction\"] = df[\"Wind_Direction\"].replace({\n",
    "        \"VARIABLE\": \"VAR\",\n",
    "        \"VAR\": \"VAR\",\n",
    "        \"CALM\": \"CALM\",\n",
    "        \"NORTH\": \"N\",\n",
    "        \"SOUTH\": \"S\",\n",
    "        \"EAST\": \"E\",\n",
    "        \"WEST\": \"W\",\n",
    "    })\n",
    "    df[\"Wind_Direction\"] = df[\"Wind_Direction\"].astype(\"category\")\n",
    "\n",
    "    # Normalise all the columns in twlight_cols to Day/Night/Unknown\n",
    "    TWILIGHT_COLS = [\"Sunrise_Sunset\",\"Civil_Twilight\",\"Nautical_Twilight\",\"Astronomical_Twilight\"]\n",
    "    for c in TWILIGHT_COLS:\n",
    "        df[c] = df[c].astype(\"string\").str.strip().str.title()\n",
    "        df.loc[~df[c].isin([\"Day\",\"Night\"]), c] = \"Unknown\"\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "    BOOL_COLS = [\n",
    "        \"Amenity\",\"Bump\",\"Crossing\",\"Give_Way\",\"Junction\",\"No_Exit\",\"Railway\",\n",
    "        \"Roundabout\",\"Station\",\"Stop\",\"Traffic_Calming\",\"Traffic_Signal\",\"Turning_Loop\"\n",
    "    ]\n",
    "    for c in BOOL_COLS:\n",
    "        if c not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Handle the case where the boolean is a string\n",
    "        if df[c].dtype.name in [\"string\", \"object\"]:\n",
    "            df[c] = df[c].str.lower().map({\"true\": True, \"false\": False})\n",
    "        df[c] = df[c].astype(\"boolean\")\n",
    "\n",
    "        # Assume all misisng boolean values to be false\n",
    "        df[c] = df[c].fillna(False)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d1a5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = clean_us_accidents(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us_accidents_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
